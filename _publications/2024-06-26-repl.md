---
title: "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks"
collection: publications
category: preprints
permalink: /publication/2024-06-26-repl
excerpt: 'We provide JUDGE-BENCH, a collection of 20 NLP datasets with human annotations, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show that each LLM exhibits a large variance across datasets in its correlation to human judgments. We conclude that LLMs are not yet ready to systematically replace human judges in NLP.'
date: 2024-06-26
venue: 'arXiv'
paperurl: 'https://arxiv.org/pdf/2406.18403'
citation: "Bavaresco, A., Bernardi, R., Bertolazzi, L., Elliott, D., Fern√°ndez, R., Gatt, A., Ghaleb, E., Giulianelli, M., Hanna, M., Koller, A. and Martins, A.F., 2024. LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks. arXiv preprint arXiv:2406.18403."
---
There is an increasing trend towards evaluating NLP models with LLM-generated judgments instead of human judgments. In the absence of a comparison against human data, this raises concerns about the validity of these evaluations; in case they are conducted with proprietary models, this also raises concerns over reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with human annotations, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show that each LLM exhibits a large variance across datasets in its correlation to human judgments. We conclude that LLMs are not yet ready to systematically replace human judges in NLP. 

[Download paper here](http://veranep.github.io/files/paper_judge.pdf)
